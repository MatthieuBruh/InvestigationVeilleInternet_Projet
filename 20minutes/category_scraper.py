from queue import Queue
from time import sleep

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait
from selenium.common.exceptions import (
    InvalidSessionIdException,
    WebDriverException,
    TimeoutException
)

from articles_scraper import scrap_article, flush_article_batch
from comments_scraper import flush_comment_batch
from dbConfig import get_connection
from utils import get_driver_requirements, accept_cookies, save_cookies

# Configuration des URLs
URLS = {
    "monde": "https://www.20min.ch/fr/monde",
    "suisse": "https://www.20min.ch/fr/suisse",
    "sport": "https://www.20min.ch/fr/sports",
    "economie": "https://www.20min.ch/fr/economie"
}


def scrape_articles_from_category(url, category):
    options, service = get_driver_requirements()
    driver = webdriver.Chrome(options=options)

    queue_art = Queue()
    try:
        print(f"\n{'=' * 60}")
        print(f"üì∞ Cat√©gorie : {category.upper()}")
        print(f"{'=' * 60}")
        print(f"Chargement de {url}")

        driver.get(url)
        accept_cookies(driver)
        save_cookies(driver, f"session_cookies_{category}.pkl")

        # Attendre que le contenu soit charg√©
        WebDriverWait(driver, 15).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "a[href^='/fr/story/']"))
        )

        # Extraire TOUS les liens d'articles
        elements = driver.find_elements(By.CSS_SELECTOR, "a[href^='/fr/story/']")
        print(f"‚Üí {len(elements)} articles trouv√©s dans {category}")

        seen_urls = set()
        for elem in elements:
            try:
                title = elem.text.strip()
                href = elem.get_attribute("href")

                if href and not href.startswith("http"):
                    href = "https://www.20min.ch" + href

                if title and href and len(title) > 10 and href not in seen_urls:
                    seen_urls.add(href)
                    queue_art.put({"title": title, "url": href})

            except Exception:
                continue

        print(f"‚Üí {queue_art.qsize()} articles uniques √† traiter\n")
        return queue_art

    except Exception as e:
        print(f"‚ùå Erreur lors du scraping de {category} : {e}")
        return None

    finally:
        try:
            driver.quit()
        except Exception:
            pass


def recreate_driver(cat):
    """Recr√©e un driver Chrome propre"""
    print(f"\nüîÑ Recr√©ation du driver Chrome...")

    options, service = get_driver_requirements()
    driver = webdriver.Chrome(options=options)

    # ‚úÖ Configurer les timeouts
    driver.set_page_load_timeout(90)
    driver.implicitly_wait(10)

    try:
        driver.get("https://www.20min.ch/fr")
        accept_cookies(driver)
        save_cookies(driver, f"session_cookies_{cat}.pkl")
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur lors de l'initialisation du driver : {e}")

    print("‚úì Driver recr√©√©\n")
    return driver


def worker_thread(article_queue, cat):
    driver = recreate_driver(cat)

    cpt = 0
    processed = 0
    failed = 0
    consecutive_errors = 0  # ‚úÖ Compteur d'erreurs cons√©cutives

    while True:
        try:
            article = article_queue.get(timeout=1)

            if article is None:
                article_queue.task_done()
                break

            try:
                print(
                    f"[{processed + failed + 1}/{article_queue.qsize() + processed + failed + 1}] Traitement : {article.get('url')}")

                scrap_article(driver, article.get('url'), cat)

                processed += 1
                consecutive_errors = 0  # Reset le compteur en cas de succ√®s

                # Petite pause entre articles
                sleep(2)

            except InvalidSessionIdException as e:
                # Session perdue ‚Üí recr√©er le driver imm√©diatement
                print(f"  ‚ö†Ô∏è Session driver perdue, recr√©ation...")

                try:
                    driver.quit()
                except:
                    pass

                driver = recreate_driver(cat)
                failed += 1
                consecutive_errors += 1

            except (TimeoutException, WebDriverException) as e:
                print(f"  ‚ùå Erreur driver/timeout : {e}")
                failed += 1
                consecutive_errors += 1

            except Exception as e:
                print(f"  ‚ùå Erreur : {e}")
                failed += 1
                consecutive_errors += 1

            article_queue.task_done()
            cpt += 1

            # ‚úÖ Si trop d'erreurs cons√©cutives ‚Üí recr√©er le driver
            if consecutive_errors >= 3:
                print(f"\n‚ö†Ô∏è 3 erreurs cons√©cutives d√©tect√©es, recr√©ation du driver...")

                # Flush avant de recr√©er
                flush_article_batch()
                flush_comment_batch()

                # Commit
                conn = get_connection()
                conn.commit()

                try:
                    driver.quit()
                except:
                    pass

                sleep(10)
                driver = recreate_driver(cat)
                consecutive_errors = 0

            # ‚úÖ Checkpoint tous les 10 articles
            if cpt % 10 == 0:
                print(f"\nüíæ Checkpoint - Sauvegarde (apr√®s {cpt} articles)")
                flush_article_batch()
                flush_comment_batch()

                conn = get_connection()
                conn.commit()
                print("‚úì Donn√©es sauvegard√©es\n")

            # ‚úÖ R√©initialisation p√©riodique tous les 20 articles
            if cpt % 20 == 0:
                print(f"\n‚è∏Ô∏è Pause - R√©initialisation du driver (apr√®s {cpt} articles)")

                # Flush avant de fermer
                flush_article_batch()
                flush_comment_batch()

                conn = get_connection()
                conn.commit()

                try:
                    driver.close()
                    driver.quit()
                except:
                    pass

                sleep(15)  # Pause plus longue

                driver = recreate_driver(cat)
                print("‚ñ∂Ô∏è Reprise du scraping\n")

        except Exception as e:
            print(f"‚ùå Erreur fatale dans worker_thread : {e}")
            break

    # ‚úÖ Nettoyage final
    print("\nüíæ Sauvegarde finale...")
    flush_article_batch()
    flush_comment_batch()

    try:
        conn = get_connection()
        conn.commit()
    except:
        pass

    try:
        driver.quit()
    except:
        pass

    print(f"\nüìä R√©sum√© {cat} :")
    print(f"  ‚úì Succ√®s : {processed}")
    print(f"  ‚úó √âchecs : {failed}")
    print(f"  Total : {processed + failed}")


def start_scraping():
    print("\n" + "=" * 60)
    print("üöÄ D√âBUT DU SCRAPING")
    print("=" * 60)

    for category, url in URLS.items():
        # ‚úÖ Retirez cette ligne pour scraper toutes les cat√©gories
        # if category == "monde":
        #     continue

        res_articles = scrape_articles_from_category(url, category)

        if res_articles and not res_articles.empty():
            worker_thread(res_articles, category)

            # ‚úÖ Flush final apr√®s chaque cat√©gorie
            print(f"\nüíæ Finalisation de la cat√©gorie {category}...")
            flush_article_batch()
            flush_comment_batch()

            try:
                conn = get_connection()
                conn.commit()
                print(f"‚úì Cat√©gorie {category} sauvegard√©e\n")
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur commit : {e}")

        else:
            print(f"‚ö†Ô∏è Aucun article trouv√© pour {category}\n")

    print("\n" + "=" * 60)
    print("‚úÖ SCRAPING TERMIN√â")
    print("=" * 60)