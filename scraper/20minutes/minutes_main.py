from minutes_article import flush_article_batch
from minutes_category import scrap_categories
from minutes_comments import flush_comment_batch
from scraper.dbConfig import close_connection

URLS = {
    "monde": "https://www.20min.ch/fr/monde",
    "suisse": "https://www.20min.ch/fr/suisse"
}

def start_scraping():
    try:
        # Lancer le scraping
        scrap_categories(URLS)
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è Interruption utilisateur d√©tect√©e")
    except Exception as e:
        print(f"\n‚ùå Erreur critique : {e}")
        import traceback
        traceback.print_exc()
    finally:
        # ‚úÖ CRITIQUE : Flush tous les batchs restants
        print("\nüíæ Sauvegarde des donn√©es restantes...")
        flush_article_batch()
        flush_comment_batch()
        # Fermer proprement la connexion
        close_connection()
        print("\n‚úÖ Programme termin√© proprement")