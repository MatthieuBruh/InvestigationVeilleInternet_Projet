import sqlite3
from detoxify import Detoxify
from datetime import datetime


def analyze_database(db_path='comments_20min.db'):
    """Analyse tous les commentaires de la base de donn√©es"""

    print("\n" + "=" * 80)
    print("üóÑÔ∏è  ANALYSE DE LA BASE DE DONN√âES")
    print("=" * 80)
    print(f"üìÅ Base de donn√©es : {db_path}")
    print(f"‚è∞ D√©marrage       : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80 + "\n")

    # Connexion √† la base de donn√©es
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()

        # R√©cup√©rer tous les commentaires
        cursor.execute("SELECT rowid, contenu FROM comments WHERE contenu IS NOT NULL AND contenu != ''")
        comments = cursor.fetchall()

        total = len(comments)
        print(f"üìä Total de commentaires trouv√©s : {total}\n")

        if total == 0:
            print("‚ö†Ô∏è Aucun commentaire √† analyser.")
            return

        # Charger le mod√®le
        print("üîÑ Chargement du mod√®le multilingue...")
        model = Detoxify('multilingual')
        print("‚úÖ Mod√®le charg√© avec succ√®s!\n")

        # Statistiques globales
        stats = {
            'total': total,
            'toxique_eleve': 0,
            'toxique_modere': 0,
            'acceptable': 0,
            'menaces': 0,
            'insultes': 0,
            'attaques_identitaires': 0
        }

        # Analyser chaque commentaire
        for i, (rowid, contenu) in enumerate(comments, 1):
            print(f"\n{'#' * 80}")
            print(f"# COMMENTAIRE {i}/{total} (ID: {rowid})")
            print(f"{'#' * 80}")

            print_category_analysis(contenu, model, stats)

            # Pause tous les 5 commentaires (optionnel)
            if i < total and i % 5 == 0:
                response = input(
                    f"\n‚è∏Ô∏è  {i}/{total} analys√©s. Continuer? (Entr√©e = oui, 'q' = arr√™ter, 's' = rapport) : ")
                if response.lower() == 'q':
                    print("\n‚èπÔ∏è  Analyse interrompue par l'utilisateur.")
                    break
                elif response.lower() == 's':
                    print_final_report(stats, i)
                    response2 = input("\nContinuer l'analyse? (Entr√©e = oui, 'q' = arr√™ter) : ")
                    if response2.lower() == 'q':
                        break

        # Rapport final
        print("\n" + "=" * 80)
        print_final_report(stats, i if 'i' in locals() else total)

        conn.close()

    except sqlite3.Error as e:
        print(f"‚ùå Erreur de base de donn√©es : {e}")
    except Exception as e:
        print(f"‚ùå Erreur : {e}")


def print_category_analysis(text, model, stats=None):
    """Analyse d√©taill√©e d'un texte"""

    print(f"\nüìù Texte analys√© :")
    # Afficher seulement les 200 premiers caract√®res si trop long
    display_text = text##[:200] + "..." if len(text) > 200 else text
    print(f"   ‚îî‚îÄ \"{display_text}\"")
    print("\n" + "-" * 80 + "\n")

    # Pr√©diction
    results = model.predict(text)

    # D√©finition des cat√©gories
    categories = {
        'toxicity': {
            'emoji': '‚ò†Ô∏è',
            'nom': 'TOXICIT√â G√âN√âRALE',
            'description': 'Score global de toxicit√©',
        },
        'severe_toxicity': {
            'emoji': 'üíÄ',
            'nom': 'TOXICIT√â S√âV√àRE',
            'description': 'Contenu tr√®s agressif',
        },
        'obscene': {
            'emoji': 'ü§¨',
            'nom': 'OBSC√âNIT√â',
            'description': 'Langage vulgaire/grossier',
        },
        'threat': {
            'emoji': '‚öîÔ∏è',
            'nom': 'MENACE',
            'description': 'Intention de nuire',
        },
        'insult': {
            'emoji': 'üò†',
            'nom': 'INSULTE',
            'description': 'Attaque personnelle',
        },
        'identity_attack': {
            'emoji': 'üë•',
            'nom': 'ATTAQUE IDENTITAIRE',
            'description': 'Discrimination',
        }
    }

    # Afficher chaque cat√©gorie (version compacte)
    for key, info in categories.items():
        score = results[key]

        # Barre de progression
        bar_length = 40
        filled = int(bar_length * score)
        bar = "‚ñà" * filled + "‚ñë" * (bar_length - filled)

        # Niveau
        if score >= 0.75:
            niveau = "üî¥"
        elif score >= 0.50:
            niveau = "üü†"
        elif score >= 0.25:
            niveau = "üü°"
        else:
            niveau = "üü¢"

        print(f"   {info['emoji']} {info['nom']:25} {score:5.1%} {niveau} [{bar}]")

    print("\n" + "-" * 80)

    # R√©sum√©
    toxicity_score = results['toxicity']

    if toxicity_score >= 0.75:
        verdict = "üö® HAUTEMENT TOXIQUE"
        if stats: stats['toxique_eleve'] += 1
    elif toxicity_score >= 0.50:
        verdict = "‚ö†Ô∏è MOD√âR√âMENT TOXIQUE"
        if stats: stats['toxique_modere'] += 1
    else:
        verdict = "‚úÖ ACCEPTABLE"
        if stats: stats['acceptable'] += 1

    print(f"\n   üèÅ Verdict : {verdict} (Toxicit√© globale: {toxicity_score:.1%})")

    # Alertes
    alerts = []
    if results['threat'] > 0.5:
        alerts.append("‚öîÔ∏è MENACE")
        if stats: stats['menaces'] += 1
    if results['insult'] > 0.7:
        alerts.append("üò† INSULTE")
        if stats: stats['insultes'] += 1
    if results['identity_attack'] > 0.6:
        alerts.append("üë• DISCRIMINATION")
        if stats: stats['attaques_identitaires'] += 1

    if alerts:
        print(f"   üîî Alertes : {' | '.join(alerts)}")


def print_final_report(stats, analyzed):
    """Affiche le rapport final des statistiques"""

    print("\n" + "=" * 80)
    print("üìà RAPPORT FINAL D'ANALYSE")
    print("=" * 80)

    print(f"\nüìä Commentaires analys√©s : {analyzed}/{stats['total']}")
    print(f"   ‚è∞ Termin√© le : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    print("\nüéØ DISTRIBUTION PAR NIVEAU DE TOXICIT√â")
    print("-" * 80)

    if analyzed > 0:
        print(
            f"   üö® Hautement toxique    : {stats['toxique_eleve']:4d} ({stats['toxique_eleve'] / analyzed * 100:5.1f}%)")
        print(
            f"   ‚ö†Ô∏è  Mod√©r√©ment toxique  : {stats['toxique_modere']:4d} ({stats['toxique_modere'] / analyzed * 100:5.1f}%)")
        print(f"   ‚úÖ Acceptable           : {stats['acceptable']:4d} ({stats['acceptable'] / analyzed * 100:5.1f}%)")

        print("\nüîî ALERTES SP√âCIFIQUES")
        print("-" * 80)
        print(f"   ‚öîÔ∏è  Menaces d√©tect√©es         : {stats['menaces']:4d}")
        print(f"   üò† Insultes d√©tect√©es         : {stats['insultes']:4d}")
        print(f"   üë• Attaques identitaires      : {stats['attaques_identitaires']:4d}")

        print("\nüí° RECOMMANDATIONS")
        print("-" * 80)

        toxic_total = stats['toxique_eleve'] + stats['toxique_modere']
        toxic_percent = toxic_total / analyzed * 100

        if toxic_percent > 30:
            print("   üö® CRITIQUE : Plus de 30% de contenu toxique d√©tect√©")
            print("   ‚Üí Mod√©ration urgente recommand√©e")
        elif toxic_percent > 15:
            print("   ‚ö†Ô∏è ATTENTION : Niveau de toxicit√© mod√©r√©")
            print("   ‚Üí Surveillance renforc√©e sugg√©r√©e")
        else:
            print("   ‚úÖ BON : Niveau de toxicit√© acceptable")
            print("   ‚Üí Maintenir la surveillance habituelle")

        if stats['menaces'] > 0:
            print(f"\n   ‚ö†Ô∏è {stats['menaces']} menace(s) d√©tect√©e(s) - Examen manuel recommand√©")

        if stats['attaques_identitaires'] > 0:
            print(f"   ‚ö†Ô∏è {stats['attaques_identitaires']} attaque(s) identitaire(s) - Possibles implications l√©gales")

    print("\n" + "=" * 80 + "\n")


# Version simple : analyse tout d'un coup
def quick_analysis(db_path='comments_20min.db', limit=None):
    """Analyse rapide sans pause, avec option de limite"""

    print(f"\nüöÄ ANALYSE RAPIDE - Base: {db_path}")

    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    query = "SELECT rowid, contenu FROM comments WHERE contenu IS NOT NULL AND contenu != ''"
    if limit:
        query += f" LIMIT {limit}"

    cursor.execute(query)
    comments = cursor.fetchall()

    print(f"üìä Analyse de {len(comments)} commentaires...\n")

    model = Detoxify('multilingual')

    stats = {
        'total': len(comments),
        'toxique_eleve': 0,
        'toxique_modere': 0,
        'acceptable': 0,
        'menaces': 0,
        'insultes': 0,
        'attaques_identitaires': 0
    }

    for i, (rowid, contenu) in enumerate(comments, 1):
        print(f"‚è≥ Progression: {i}/{len(comments)}", end='\r')

        results = model.predict(contenu)

        # Mise √† jour des stats
        if results['toxicity'] >= 0.75:
            stats['toxique_eleve'] += 1
        elif results['toxicity'] >= 0.50:
            stats['toxique_modere'] += 1
        else:
            stats['acceptable'] += 1

        if results['threat'] > 0.5:
            stats['menaces'] += 1
        if results['insult'] > 0.7:
            stats['insultes'] += 1
        if results['identity_attack'] > 0.6:
            stats['attaques_identitaires'] += 1

    print("\n")
    print_final_report(stats, len(comments))

    conn.close()


# Utilisation
if __name__ == "__main__":

    print("\nüîç ANALYSEUR DE COMMENTAIRES - D√©tection de toxicit√©")
    print("\nChoisissez le mode d'analyse :")
    print("1. Analyse d√©taill√©e (affiche chaque commentaire)")
    print("2. Analyse rapide (statistiques uniquement)")
    print("3. Test rapide (10 premiers commentaires)")

    choix = input("\nVotre choix (1/2/3) : ")

    if choix == "1":
        analyze_database('comments_20min.db')
    elif choix == "2":
        quick_analysis('comments_20min.db')
    elif choix == "3":
        quick_analysis('comments_20min.db', limit=10)
    else:
        print("‚ùå Choix invalide")