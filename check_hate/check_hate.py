# Load model directly
'''import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

tokenizer = AutoTokenizer.from_pretrained("Hate-speech-CNERG/new-counterspeech-score")
model = AutoModelForSequenceClassification.from_pretrained("Hate-speech-CNERG/new-counterspeech-score")

# Exemple de texte
text = "Il faut renvoyer ces sales nÃ¨gres pro hamas, qu'ils aillent se faire buter chez ces singes"

inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
outputs = model(**inputs)

# Obtenir les prÃ©dictions
predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
predicted_class = torch.argmax(predictions, dim=-1).item()

# Les labels sont: 0 = NON_HATE, 1 = HATE
labels = ["NON_HATE", "HATE"]
print(f"PrÃ©diction: {labels[predicted_class]}")
print(f"Confiance: {predictions[0][predicted_class].item():.4f}")

from transformers import pipeline


# CrÃ©er le pipeline
classifier = pipeline("text-classification",
                     model="Hate-speech-CNERG/new-counterspeech-score")

# Analyser un texte
result = classifier("J'aime al vie")
print(result)

from detoxify import Detoxify

# Utiliser le modÃ¨le multilingue
model = Detoxify('multilingual')

# Tester sur plusieurs langues
results = model.predict("Il faut Ãªtre psychologiquement atteint pour aller lÃ -bas.")

print(results)'''

from detoxify import Detoxify


def print_category_analysis(text):
    """Analyse dÃ©taillÃ©e avec explication de chaque catÃ©gorie"""

    print("\n" + "=" * 80)
    print("ðŸ” ANALYSE DÃ‰TAILLÃ‰E DU TEXTE")
    print("=" * 80)

    print(f"\nðŸ“ Texte analysÃ© :")
    print(f"   â””â”€ \"{text}\"")
    print("\n" + "-" * 80 + "\n")

    # PrÃ©diction
    model = Detoxify('multilingual')
    results = model.predict(text)

    # DÃ©finition des catÃ©gories avec emoji, nom et explication
    categories = {
        'toxicity': {
            'emoji': 'â˜ ï¸',
            'nom': 'TOXICITÃ‰ GÃ‰NÃ‰RALE',
            'description': 'Score global de toxicitÃ© (impoli, irrespectueux, dÃ©sagrÃ©able)',
            'exemples': 'Commentaires mÃ©chants, propos blessants'
        },
        'severe_toxicity': {
            'emoji': 'ðŸ’€',
            'nom': 'TOXICITÃ‰ SÃ‰VÃˆRE',
            'description': 'Contenu trÃ¨s agressif qui repousserait fortement les gens',
            'exemples': 'Haine extrÃªme, violence verbale intense'
        },
        'obscene': {
            'emoji': 'ðŸ¤¬',
            'nom': 'OBSCÃ‰NITÃ‰',
            'description': 'Langage vulgaire, grossier ou sexuellement explicite',
            'exemples': 'Gros mots, insultes crues, contenu sexuel'
        },
        'threat': {
            'emoji': 'âš”ï¸',
            'nom': 'MENACE',
            'description': 'Intention de nuire physiquement, financiÃ¨rement ou autrement',
            'exemples': '"Je vais te frapper", "Tu vas le payer"'
        },
        'insult': {
            'emoji': 'ðŸ˜ ',
            'nom': 'INSULTE',
            'description': 'Attaque personnelle, commentaire offensant ou dÃ©gradant',
            'exemples': '"Tu es stupide", "Quel idiot"'
        },
        'identity_attack': {
            'emoji': 'ðŸ‘¥',
            'nom': 'ATTAQUE IDENTITAIRE',
            'description': 'Discrimination basÃ©e sur race, religion, genre, orientation, etc.',
            'exemples': 'Racisme, sexisme, homophobie, xÃ©nophobie'
        }
    }

    # Afficher chaque catÃ©gorie
    for key, info in categories.items():
        score = results[key]

        print(f"{info['emoji']} {info['nom']}")
        print("-" * 80)

        # Barre de progression
        bar_length = 50
        filled = int(bar_length * score)
        bar = "â–ˆ" * filled + "â–‘" * (bar_length - filled)

        # Couleur textuelle selon le score
        if score >= 0.75:
            niveau = "ðŸ”´ Ã‰LEVÃ‰"
            color_bar = f"\033[91m{bar}\033[0m"  # Rouge
        elif score >= 0.50:
            niveau = "ðŸŸ  MODÃ‰RÃ‰"
            color_bar = f"\033[93m{bar}\033[0m"  # Jaune
        elif score >= 0.25:
            niveau = "ðŸŸ¡ FAIBLE"
            color_bar = f"\033[93m{bar}\033[0m"  # Jaune
        else:
            niveau = "ðŸŸ¢ TRÃˆS FAIBLE"
            color_bar = f"\033[92m{bar}\033[0m"  # Vert

        print(f"   Score    : {score:.1%} {niveau}")
        print(f"   Barre    : [{bar}]")
        print(f"   ðŸ“– Info  : {info['description']}")
        print(f"   ðŸ’­ Ex.   : {info['exemples']}")
        print()

    print("-" * 80)

    # RÃ©sumÃ© global
    print("\nðŸ“Š RÃ‰SUMÃ‰")
    print("-" * 80)

    # Trouver la catÃ©gorie dominante
    max_key = max(results, key=results.get)
    max_score = results[max_key]
    max_info = categories[max_key]

    print(f"   ðŸŽ¯ CatÃ©gorie principale : {max_info['emoji']} {max_info['nom']}")
    print(f"   ðŸ“ˆ Score maximum        : {max_score:.1%}")
    print()

    # Ã‰valuation globale
    toxicity_score = results['toxicity']

    if toxicity_score >= 0.75:
        verdict = "ðŸš¨ CONTENU HAUTEMENT TOXIQUE - Action recommandÃ©e"
    elif toxicity_score >= 0.50:
        verdict = "âš ï¸ CONTENU MODÃ‰RÃ‰MENT TOXIQUE - VÃ©rification suggÃ©rÃ©e"
    elif toxicity_score >= 0.25:
        verdict = "âš¡ CONTENU LÃ‰GÃˆREMENT TOXIQUE - Surveillance"
    else:
        verdict = "âœ… CONTENU ACCEPTABLE"

    print(f"   ðŸ Verdict global       : {verdict}")

    # Alertes spÃ©cifiques
    print(f"\n   ðŸ”” Alertes spÃ©cifiques  :")
    alerts = []

    if results['threat'] > 0.5:
        alerts.append("      â€¢ MENACE dÃ©tectÃ©e - PrioritÃ© Ã©levÃ©e")
    if results['severe_toxicity'] > 0.5:
        alerts.append("      â€¢ TOXICITÃ‰ SÃ‰VÃˆRE - NÃ©cessite attention")
    if results['identity_attack'] > 0.6:
        alerts.append("      â€¢ DISCRIMINATION dÃ©tectÃ©e - Possibles implications lÃ©gales")
    if results['insult'] > 0.7:
        alerts.append("      â€¢ INSULTES rÃ©pÃ©tÃ©es")
    if results['obscene'] > 0.7:
        alerts.append("      â€¢ LANGAGE VULGAIRE excessif")

    if alerts:
        for alert in alerts:
            print(alert)
    else:
        print("      â€¢ Aucune alerte critique")

    print("\n" + "=" * 80 + "\n")


# Exemples d'utilisation
if __name__ == "__main__":

    exemples = [""]

    for i, texte in enumerate(exemples, 1):
        print(f"\n{'#' * 80}")
        print(f"# EXEMPLE {i}/{len(exemples)}")
        print(f"{'#' * 80}")
        print_category_analysis(texte)

        if i < len(exemples):
            input("\nâ¸ï¸  Appuyez sur EntrÃ©e pour voir l'exemple suivant...\n")