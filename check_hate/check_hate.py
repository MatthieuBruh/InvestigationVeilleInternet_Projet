# Load model directly
'''import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

tokenizer = AutoTokenizer.from_pretrained("Hate-speech-CNERG/new-counterspeech-score")
model = AutoModelForSequenceClassification.from_pretrained("Hate-speech-CNERG/new-counterspeech-score")

# Exemple de texte
text = "Il faut renvoyer ces sales n√®gres pro hamas, qu'ils aillent se faire buter chez ces singes"

inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
outputs = model(**inputs)

# Obtenir les pr√©dictions
predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
predicted_class = torch.argmax(predictions, dim=-1).item()

# Les labels sont: 0 = NON_HATE, 1 = HATE
labels = ["NON_HATE", "HATE"]
print(f"Pr√©diction: {labels[predicted_class]}")
print(f"Confiance: {predictions[0][predicted_class].item():.4f}")

from transformers import pipeline


# Cr√©er le pipeline
classifier = pipeline("text-classification",
                     model="Hate-speech-CNERG/new-counterspeech-score")

# Analyser un texte
result = classifier("J'aime al vie")
print(result)

from detoxify import Detoxify

# Utiliser le mod√®le multilingue
model = Detoxify('multilingual')

# Tester sur plusieurs langues
results = model.predict("Il faut √™tre psychologiquement atteint pour aller l√†-bas.")

print(results)'''

from detoxify import Detoxify


def print_category_analysis(text):
    """Analyse d√©taill√©e avec explication de chaque cat√©gorie"""

    print("\n" + "=" * 80)
    print("üîç ANALYSE D√âTAILL√âE DU TEXTE")
    print("=" * 80)

    print(f"\nüìù Texte analys√© :")
    print(f"   ‚îî‚îÄ \"{text}\"")
    print("\n" + "-" * 80 + "\n")

    # Pr√©diction
    model = Detoxify('multilingual')
    results = model.predict(text)

    # D√©finition des cat√©gories avec emoji, nom et explication
    categories = {
        'toxicity': {
            'emoji': '‚ò†Ô∏è',
            'nom': 'TOXICIT√â G√âN√âRALE',
            'description': 'Score global de toxicit√© (impoli, irrespectueux, d√©sagr√©able)',
            'exemples': 'Commentaires m√©chants, propos blessants'
        },
        'severe_toxicity': {
            'emoji': 'üíÄ',
            'nom': 'TOXICIT√â S√âV√àRE',
            'description': 'Contenu tr√®s agressif qui repousserait fortement les gens',
            'exemples': 'Haine extr√™me, violence verbale intense'
        },
        'obscene': {
            'emoji': 'ü§¨',
            'nom': 'OBSC√âNIT√â',
            'description': 'Langage vulgaire, grossier ou sexuellement explicite',
            'exemples': 'Gros mots, insultes crues, contenu sexuel'
        },
        'threat': {
            'emoji': '‚öîÔ∏è',
            'nom': 'MENACE',
            'description': 'Intention de nuire physiquement, financi√®rement ou autrement',
            'exemples': '"Je vais te frapper", "Tu vas le payer"'
        },
        'insult': {
            'emoji': 'üò†',
            'nom': 'INSULTE',
            'description': 'Attaque personnelle, commentaire offensant ou d√©gradant',
            'exemples': '"Tu es stupide", "Quel idiot"'
        },
        'identity_attack': {
            'emoji': 'üë•',
            'nom': 'ATTAQUE IDENTITAIRE',
            'description': 'Discrimination bas√©e sur race, religion, genre, orientation, etc.',
            'exemples': 'Racisme, sexisme, homophobie, x√©nophobie'
        }
    }

    # Afficher chaque cat√©gorie
    for key, info in categories.items():
        score = results[key]

        print(f"{info['emoji']} {info['nom']}")
        print("-" * 80)

        # Barre de progression
        bar_length = 50
        filled = int(bar_length * score)
        bar = "‚ñà" * filled + "‚ñë" * (bar_length - filled)

        # Couleur textuelle selon le score
        if score >= 0.75:
            niveau = "üî¥ √âLEV√â"
            color_bar = f"\033[91m{bar}\033[0m"  # Rouge
        elif score >= 0.50:
            niveau = "üü† MOD√âR√â"
            color_bar = f"\033[93m{bar}\033[0m"  # Jaune
        elif score >= 0.25:
            niveau = "üü° FAIBLE"
            color_bar = f"\033[93m{bar}\033[0m"  # Jaune
        else:
            niveau = "üü¢ TR√àS FAIBLE"
            color_bar = f"\033[92m{bar}\033[0m"  # Vert

        print(f"   Score    : {score:.1%} {niveau}")
        print(f"   Barre    : [{bar}]")
        print(f"   üìñ Info  : {info['description']}")
        print(f"   üí≠ Ex.   : {info['exemples']}")
        print()

    print("-" * 80)

    # R√©sum√© global
    print("\nüìä R√âSUM√â")
    print("-" * 80)

    # Trouver la cat√©gorie dominante
    max_key = max(results, key=results.get)
    max_score = results[max_key]
    max_info = categories[max_key]

    print(f"   üéØ Cat√©gorie principale : {max_info['emoji']} {max_info['nom']}")
    print(f"   üìà Score maximum        : {max_score:.1%}")
    print()

    # √âvaluation globale
    toxicity_score = results['toxicity']

    if toxicity_score >= 0.75:
        verdict = "üö® CONTENU HAUTEMENT TOXIQUE - Action recommand√©e"
    elif toxicity_score >= 0.50:
        verdict = "‚ö†Ô∏è CONTENU MOD√âR√âMENT TOXIQUE - V√©rification sugg√©r√©e"
    elif toxicity_score >= 0.25:
        verdict = "‚ö° CONTENU L√âG√àREMENT TOXIQUE - Surveillance"
    else:
        verdict = "‚úÖ CONTENU ACCEPTABLE"

    print(f"   üèÅ Verdict global       : {verdict}")

    # Alertes sp√©cifiques
    print(f"\n   üîî Alertes sp√©cifiques  :")
    alerts = []

    if results['threat'] > 0.5:
        alerts.append("      ‚Ä¢ MENACE d√©tect√©e - Priorit√© √©lev√©e")
    if results['severe_toxicity'] > 0.5:
        alerts.append("      ‚Ä¢ TOXICIT√â S√âV√àRE - N√©cessite attention")
    if results['identity_attack'] > 0.6:
        alerts.append("      ‚Ä¢ DISCRIMINATION d√©tect√©e - Possibles implications l√©gales")
    if results['insult'] > 0.7:
        alerts.append("      ‚Ä¢ INSULTES r√©p√©t√©es")
    if results['obscene'] > 0.7:
        alerts.append("      ‚Ä¢ LANGAGE VULGAIRE excessif")

    if alerts:
        for alert in alerts:
            print(alert)
    else:
        print("      ‚Ä¢ Aucune alerte critique")

    print("\n" + "=" * 80 + "\n")


# Exemples d'utilisation
if __name__ == "__main__":

    exemples = ["La gauche √©colo va revenir √† la charge avec leur caisse unique. Caisse unique avec primes calcul√©es selon le revenu. Cela aura pour cons√©quence que les m√™mes vont payer pour pouvoir fiancer encore d'avantage d'assist√©s qui ne paieront rien. Regardez en France le d√©sastre de leur Caisse maladie unique. Manque de m√©decins, Manque d'h√¥pitaux. Syst√®me de sant√© d√©labr√©e. La Caisse unique c'est une chose. Mais grand jamais, Non et Non √† un imp√¥t d√©guis√© qui plume les assur√©s et fait payer les primes selon le revenu-fortune de l'assur√©. Non √† cette nouvelle arnaque des gauchos et partie des droitards-centristes. "]

    for i, texte in enumerate(exemples, 1):
        print(f"\n{'#' * 80}")
        print(f"# EXEMPLE {i}/{len(exemples)}")
        print(f"{'#' * 80}")
        print_category_analysis(texte)

        if i < len(exemples):
            input("\n‚è∏Ô∏è  Appuyez sur Entr√©e pour voir l'exemple suivant...\n")